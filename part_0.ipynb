{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting and Joining Strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"let's\", 'go', 'to', 'N.y.!']\n",
      "let'sgotoN.y.!\n"
     ]
    }
   ],
   "source": [
    "sentence = \"let's go to N.y!\"\n",
    "words = sentence.split()\n",
    "print(words)\n",
    "joined_sentence = ' '.join(words)\n",
    "print(joined_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replacing Substrings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I love programming in Java\n"
     ]
    }
   ],
   "source": [
    "sentence = \"I love programming in Python\"\n",
    "new_sentence = sentence.replace(\"Python\", \"Java\")\n",
    "print(new_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Introduction to Regular Expressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 7\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "sentence = \"The cat sat on the mat\"\n",
    "match = re.search(r\"cat\", sentence)\n",
    "print(match.start(), match.end())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting a String at a Specific Character"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I like apples', 'oranges', 'and bananas']\n"
     ]
    }
   ],
   "source": [
    "sentence = \"I like apples, oranges, and bananas\"\n",
    "fruits = sentence.split(\", \")\n",
    "print(fruits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reversing a String"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing Whitespace from a String"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!dlroW ,olleH\n"
     ]
    }
   ],
   "source": [
    "text = \"Hello, World!\"\n",
    "reversed_text = text[::-1]\n",
    "print(reversed_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thisisasentencewithextraspaces.\n"
     ]
    }
   ],
   "source": [
    "text = \"  This is a sentence with  extra spaces.   \"\n",
    "cleaned_text = text.replace(\" \", \"\")\n",
    "print(cleaned_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token: Welcome | Next words: ['to', 'cognitive', 'science', 'lab', '.']\n",
      "Token: to | Next words: ['cognitive', 'science', 'lab', '.', 'Todays']\n",
      "Token: cognitive | Next words: ['science', 'lab', '.', 'Todays', 'lab']\n",
      "Token: science | Next words: ['lab', '.', 'Todays', 'lab', 'class']\n",
      "Token: lab | Next words: ['.', 'Todays', 'lab', 'class', 'would']\n",
      "Token: . | Next words: ['Todays', 'lab', 'class', 'would', 'be']\n",
      "Token: Todays | Next words: ['lab', 'class', 'would', 'be', 'about']\n",
      "Token: lab | Next words: ['class', 'would', 'be', 'about', 'NLP']\n",
      "Token: class | Next words: ['would', 'be', 'about', 'NLP', '.']\n",
      "Token: would | Next words: ['be', 'about', 'NLP', '.', 'On']\n",
      "Token: be | Next words: ['about', 'NLP', '.', 'On', 'this']\n",
      "Token: about | Next words: ['NLP', '.', 'On', 'this', 'experiment']\n",
      "Token: NLP | Next words: ['.', 'On', 'this', 'experiment', 'we']\n",
      "Token: . | Next words: ['On', 'this', 'experiment', 'we', 'do']\n",
      "Token: On | Next words: ['this', 'experiment', 'we', 'do', 'tokenization']\n",
      "Token: this | Next words: ['experiment', 'we', 'do', 'tokenization', 'experiment']\n",
      "Token: experiment | Next words: ['we', 'do', 'tokenization', 'experiment', '.']\n",
      "Token: we | Next words: ['do', 'tokenization', 'experiment', '.']\n",
      "Token: do | Next words: ['tokenization', 'experiment', '.']\n",
      "Token: tokenization | Next words: ['experiment', '.']\n",
      "Token: experiment | Next words: ['.']\n",
      "Token: . | Next words: []\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "text = text = \"\"\"Welcome to cognitive science lab. Todays lab class would be about NLP. On this experiment we do tokenization experiment.\"\"\"\n",
    "doc = nlp(text)\n",
    "for token in doc:\n",
    "    print(f\"Token: {token.text} | Next words: {[token.text for token in token.doc[token.i+1:token.i+6]]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
